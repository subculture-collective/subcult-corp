import type { DirectoryEntry } from './types';

export const directory: DirectoryEntry[] = [
    {
        slug: 'openclaw',
        name: 'OpenClaw',
        shortDesc:
            'Open-source AI gateway that connects agents to tools and messaging channels.',
        body: 'OpenClaw is an open-source gateway for AI agents. It provides a bridge between your agent logic and external tools (called skills), exposed over an OpenAI-compatible HTTP API and WebSocket connections.\n\nThe gateway supports multiple messaging channels — Discord, Telegram, WhatsApp, Mattermost — letting agents interact with users across platforms. Skills run in sandboxed environments, and the gateway handles authentication, rate limiting, and tool discovery.\n\nSUBCULT OPS uses OpenClaw to give its six agents access to web search, file operations, and other capabilities. The gateway runs as a lightweight systemd service with minimal configuration.',
        category: 'platform',
        pricing: 'free',
        url: 'https://github.com/open-claw/openclaw',
        features: [
            'OpenAI-compatible HTTP API',
            'WebSocket primary protocol',
            'Multi-channel support (Discord, Telegram, WhatsApp)',
            'Skill sandboxing and execution',
            'Password and token authentication',
        ],
        pros: [
            'Single binary, easy deployment',
            'Open source and self-hosted',
            'Compatible with any OpenAI-speaking agent',
            'Multi-channel out of the box',
        ],
        cons: [
            'Skills execute via chat, not direct API calls',
            'Smaller community than major frameworks',
            'Documentation still evolving',
        ],
        related: ['anthropic-mcp', 'langchain', 'openrouter'],
    },
    {
        slug: 'claude',
        name: 'Claude (Anthropic)',
        shortDesc:
            'Frontier LLM family by Anthropic — known for long context, safety, and instruction following.',
        body: "Claude is Anthropic's family of large language models. The lineup includes Opus (most capable), Sonnet (balanced), and Haiku (fastest). Claude models are known for strong instruction following, 200K token context windows, and Constitutional AI safety training.\n\nFor agent systems, Claude offers tool use via tool_use content blocks, extended thinking mode for complex reasoning, and reliable structured output. The API is available directly from Anthropic or through aggregators like OpenRouter.\n\nClaude excels at long-document analysis, careful reasoning, and tasks that require following complex multi-step instructions — all critical capabilities for autonomous agents.",
        category: 'model-provider',
        pricing: 'paid',
        url: 'https://anthropic.com',
        features: [
            '200K token context window',
            'Tool use via content blocks',
            'Extended thinking mode',
            'Vision and document analysis',
            'Constitutional AI safety',
        ],
        pros: [
            'Excellent instruction following',
            'Long effective context window',
            'Strong at analysis and reasoning',
            'Reliable tool use',
        ],
        cons: [
            'Smaller ecosystem than OpenAI',
            'No image generation',
            'Fewer fine-tuning options',
        ],
        related: ['openai-gpt', 'openrouter', 'anthropic-mcp'],
    },
    {
        slug: 'openai-gpt',
        name: 'OpenAI GPT',
        shortDesc:
            'The GPT model family — pioneered modern LLMs with the largest ecosystem of tools and integrations.',
        body: "OpenAI's GPT family (GPT-4o, GPT-4, o1, o3) defined the modern LLM landscape. GPT models pioneered function calling, the Assistants API, and many patterns that became industry standards.\n\nThe ecosystem around GPT is the largest in the industry — thousands of plugins, integrations, and tools work with OpenAI's API format. Most agent frameworks default to OpenAI-compatible APIs, making GPT the most broadly supported model family.\n\nGPT-4o offers strong multimodal capabilities (text, vision, audio), while the o-series models provide enhanced reasoning. The Assistants API adds built-in code execution, file handling, and retrieval.",
        category: 'model-provider',
        pricing: 'paid',
        url: 'https://openai.com',
        features: [
            'Pioneered function calling pattern',
            'Assistants API with built-in tools',
            'Multimodal (text, vision, audio, image gen)',
            'Fine-tuning and batch API',
            'Largest third-party ecosystem',
        ],
        pros: [
            'Broadest compatibility and ecosystem',
            'Strong code generation',
            'Mature function calling',
            'Multiple model tiers for cost optimization',
        ],
        cons: [
            'Shorter context than Claude (128K vs 200K)',
            'Less literal instruction following',
            'Ecosystem lock-in concerns',
        ],
        related: ['claude', 'openrouter', 'ollama'],
    },
    {
        slug: 'langchain',
        name: 'LangChain',
        shortDesc:
            'The most popular framework for building LLM applications — chains, agents, RAG, and more.',
        body: "LangChain is the most widely adopted framework for building applications powered by language models. It provides abstractions for chains (sequential LLM calls), agents (autonomous tool-using systems), RAG (retrieval-augmented generation), and memory.\n\nLangGraph, LangChain's agent orchestration layer, enables building stateful, multi-step agent workflows as graphs. LangSmith provides observability and tracing for debugging agent behavior in production.\n\nThe framework supports 700+ integrations with tools, vector stores, LLM providers, and data sources. While it adds abstraction overhead, it significantly speeds up development for complex LLM applications.",
        category: 'framework',
        pricing: 'free',
        url: 'https://langchain.com',
        features: [
            'LangGraph for stateful agent workflows',
            'LangSmith for tracing and observability',
            '700+ integrations',
            'Built-in RAG and memory patterns',
            'Multi-language (Python, JavaScript/TypeScript)',
        ],
        pros: [
            'Largest ecosystem and community',
            'Comprehensive documentation',
            'Rapid prototyping',
            'Production-ready with LangSmith',
        ],
        cons: [
            'Heavy abstraction layer',
            'Frequent breaking changes',
            'Can be over-engineered for simple use cases',
        ],
        related: ['autogen', 'crewai', 'vercel-ai-sdk'],
    },
    {
        slug: 'autogen',
        name: 'AutoGen',
        shortDesc:
            'Microsoft\'s multi-agent conversation framework — built for agent-to-agent dialogue and coordination.',
        body: "AutoGen is Microsoft's open-source framework specifically designed for multi-agent conversations. Unlike general-purpose LLM frameworks, AutoGen was built from the ground up around the concept of agents talking to each other.\n\nThe framework provides primitives for agent-to-agent messaging, group chat with dynamic speaker selection, human-in-the-loop patterns, and code execution. AutoGen agents can be backed by different LLMs, allowing you to use the right model for each agent role.\n\nAutoGen is particularly strong for research-oriented multi-agent systems where agent dialogue and debate are core to the workflow.",
        category: 'framework',
        pricing: 'free',
        url: 'https://github.com/microsoft/autogen',
        features: [
            'Native multi-agent conversation support',
            'Group chat with speaker selection',
            'Human-in-the-loop integration',
            'Code execution environment',
            'Flexible agent configuration',
        ],
        pros: [
            'Purpose-built for multi-agent systems',
            'Microsoft research backing',
            'Clean conversation primitives',
            'Good for debate and verification patterns',
        ],
        cons: [
            'Primarily Python',
            'Smaller ecosystem than LangChain',
            'Less mature tooling ecosystem',
        ],
        related: ['langchain', 'crewai', 'openclaw'],
    },
    {
        slug: 'crewai',
        name: 'CrewAI',
        shortDesc:
            'Role-based multi-agent framework — define agents with roles, goals, and backstories, then let them collaborate.',
        body: "CrewAI takes a role-playing approach to multi-agent systems. You define agents with specific roles, goals, and backstories, then organize them into crews that execute tasks collaboratively.\n\nThe framework emphasizes simplicity — you describe what each agent should do in natural language, and CrewAI handles the orchestration. This makes it accessible to developers who want multi-agent capabilities without deep framework knowledge.\n\nCrewAI supports sequential and hierarchical task execution, inter-agent delegation, and memory. It integrates with LangChain tools, giving you access to a broad ecosystem of capabilities.",
        category: 'framework',
        pricing: 'freemium',
        url: 'https://crewai.com',
        features: [
            'Role-based agent definition',
            'Natural language task descriptions',
            'Sequential and hierarchical execution',
            'Inter-agent delegation',
            'LangChain tool compatibility',
        ],
        pros: [
            'Intuitive role-based design',
            'Low learning curve',
            'Good for well-defined workflows',
            'Active community',
        ],
        cons: [
            'Less flexible than lower-level frameworks',
            'Python only',
            'Limited customization of agent interaction patterns',
        ],
        related: ['autogen', 'langchain', 'openclaw'],
    },
    {
        slug: 'openrouter',
        name: 'OpenRouter',
        shortDesc:
            'Unified API gateway for 200+ LLMs — one endpoint, all providers, with built-in routing and fallback.',
        body: "OpenRouter aggregates language models from every major provider behind a single OpenAI-compatible API. Send requests to one endpoint, specify any model, and OpenRouter handles the rest — provider selection, rate limiting, and billing.\n\nFor agent systems, OpenRouter's killer feature is the models array: specify multiple models and the API automatically falls back to the next one if the first fails. Combined with provider routing (choosing the fastest or cheapest endpoint for a model), this gives your agents high availability without complex client-side logic.\n\nThe platform tracks usage across all models in one dashboard, making it easy to optimize costs and identify which models perform best for your use cases.",
        category: 'platform',
        pricing: 'paid',
        url: 'https://openrouter.ai',
        features: [
            '200+ models from all major providers',
            'Native model fallback with models array',
            'Provider routing for cost/latency optimization',
            'Unified billing and usage tracking',
            'OpenAI-compatible API format',
        ],
        pros: [
            'One API key for all models',
            'Built-in fallback routing',
            'Competitive pricing',
            'Great for multi-model systems',
        ],
        cons: [
            'Additional latency from proxy layer',
            'New features may lag behind direct APIs',
            'Dependency on third-party service',
        ],
        related: ['ollama', 'claude', 'openai-gpt'],
    },
    {
        slug: 'ollama',
        name: 'Ollama',
        shortDesc:
            'Run open-source LLMs locally — download, serve, and use models on your own hardware with a simple CLI.',
        body: "Ollama makes local LLM deployment simple. Install it, pull a model, and you have a running inference server. It supports dozens of open-source models including Llama, Mistral, Qwen, DeepSeek, and Gemma.\n\nThe tool handles model management (downloading, updating, quantization), GPU acceleration (NVIDIA CUDA, Apple Metal), and serving via both a native API and an OpenAI-compatible endpoint. This makes it easy to integrate with existing tools and frameworks.\n\nFor agent systems, Ollama provides a privacy-preserving, zero-cost-per-token option for simpler tasks. Combined with cloud models for complex reasoning, it enables hybrid architectures that balance cost, privacy, and capability.",
        category: 'platform',
        pricing: 'free',
        url: 'https://ollama.com',
        features: [
            'One-command model download and serving',
            'Native and OpenAI-compatible APIs',
            'GPU acceleration (NVIDIA, Apple Silicon)',
            'Model customization via Modelfiles',
            'Supports dozens of open-source models',
        ],
        pros: [
            'Complete data privacy',
            'Zero per-token cost',
            'No internet dependency',
            'Simple CLI interface',
        ],
        cons: [
            'Requires capable hardware',
            'Models less capable than cloud frontier',
            'Limited tool use reliability',
        ],
        related: ['openrouter', 'openai-gpt', 'claude'],
    },
    {
        slug: 'anthropic-mcp',
        name: 'Anthropic MCP',
        shortDesc:
            'Model Context Protocol — an open standard for connecting AI to external tools and data sources.',
        body: "The Model Context Protocol (MCP) by Anthropic is an open standard that defines how AI applications connect to external tools and data. It provides a client-server architecture where MCP clients (AI apps) discover and use tools exposed by MCP servers.\n\nMCP standardizes tool discovery, invocation, and response handling. A tool written as an MCP server works with any MCP-compatible client — Claude Desktop, VS Code extensions, agent frameworks, and more. This eliminates the need for custom integrations per tool per client.\n\nThe protocol also supports resources (read-only data sources) and prompts (reusable prompt templates), making it a comprehensive interface between AI and external systems.",
        category: 'protocol',
        pricing: 'free',
        url: 'https://modelcontextprotocol.io',
        features: [
            'Standardized tool discovery and invocation',
            'Client-server architecture',
            'Resources for read-only data access',
            'Prompt templates',
            'Growing ecosystem of MCP servers',
        ],
        pros: [
            'Open standard — no vendor lock-in',
            'Write once, use everywhere',
            'Growing ecosystem',
            'Backed by Anthropic',
        ],
        cons: [
            'Still relatively new',
            'Not all tools have MCP servers yet',
            'Some frameworks have limited MCP support',
        ],
        related: ['openclaw', 'claude', 'langchain'],
    },
    {
        slug: 'vercel-ai-sdk',
        name: 'Vercel AI SDK',
        shortDesc:
            'TypeScript toolkit for building AI applications — streaming, tool use, and multi-provider support for React and Next.js.',
        body: "The Vercel AI SDK is a TypeScript-first toolkit for building AI-powered applications, particularly with React and Next.js. It provides hooks and utilities for streaming LLM responses, managing conversations, handling tool calls, and rendering AI-generated UI.\n\nThe SDK supports multiple LLM providers through a unified interface, with first-class support for OpenAI, Anthropic, Google, and others. Its streaming architecture is optimized for web applications, providing real-time response rendering.\n\nFor TypeScript developers building web-facing AI applications, the Vercel AI SDK offers the tightest integration with the React/Next.js ecosystem. It handles the complexity of streaming, tool use, and multi-step agent workflows.",
        category: 'framework',
        pricing: 'free',
        url: 'https://sdk.vercel.ai',
        features: [
            'React hooks for streaming AI responses',
            'Multi-provider support (OpenAI, Anthropic, Google)',
            'Tool use and function calling',
            'Generative UI components',
            'Built for Next.js and React Server Components',
        ],
        pros: [
            'TypeScript-first design',
            'Excellent Next.js integration',
            'Great streaming UX',
            'Active development by Vercel',
        ],
        cons: [
            'Web/React focused — less suitable for backend-only agents',
            'Smaller ecosystem than LangChain',
            'Tightly coupled to Vercel ecosystem',
        ],
        related: ['langchain', 'openrouter', 'anthropic-mcp'],
    },
];

export const directoryLabelMap: Record<string, string> = Object.fromEntries(
    directory.map(e => [e.slug, e.name])
);
